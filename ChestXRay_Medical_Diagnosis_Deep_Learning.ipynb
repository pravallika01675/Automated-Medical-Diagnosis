{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tks9e_G7fzRX"
   },
   "source": [
    "# Chest X-Ray Medical Diagnosis with Deep Learning\n",
    "\n",
    "# Model Interpretation Methods\n",
    "\n",
    "<img src=\"xray-header-image.png\" style=\"padding-top: 50px;width: 87%;left: 0px;margin-left: 0px;margin-right: 0px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "i9OcyAhSesQc",
    "outputId": "b32cbd74-6f95-476e-e0db-c5739e886d21"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/NAGA PRASAD/AppData/Local/Microsoft/WindowsApps/python3.11.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary packages\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sklearn\n",
    "import shap\n",
    "import os\n",
    "import seaborn as sns\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# This sets a common size for all the figures we will draw.\n",
    "plt.rcParams['figure.figsize'] = [10, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "we will work with chest x-ray images taken from the public [ChestX-ray8 dataset](https://arxiv.org/abs/1705.02315). In this notebook, you'll get a chance to explore this dataset and familiarize yourself with .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv file containing training datadata\n",
    "train_df = pd.read_csv(\"nih_new/train-small.csv\")\n",
    "valid_df = pd.read_csv(\"nih_new/valid-small.csv\")\n",
    "test_df = pd.read_csv(\"nih_new/test.csv\")\n",
    "print(f'There are {train_df.shape[0]} rows and {train_df.shape[1]} columns in the train data frame')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train set: The total patient ids are {train_df['PatientId'].count()}, from those the unique ids are {train_df['PatientId'].value_counts().shape[0]} \")\n",
    "print(f\"Validation set: The total patient ids are {valid_df['PatientId'].count()}\")\n",
    "print(f\"Test set: The total patient ids are {test_df['PatientId'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preventing Data Leakage\n",
    "It is worth noting that our dataset contains multiple images for each patient. This could be the case, for example, when a patient has taken multiple X-ray images at different times during their hospital visits. In our data splitting, we have ensured that the split is done on the patient level so that there is no data \"leakage\" between the train, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_leakage(df1, df2, patient_col):\n",
    "    \"\"\"\n",
    "    Return True if there any patients are in both df1 and df2.\n",
    "\n",
    "    Args:\n",
    "        df1 (dataframe): dataframe describing first dataset\n",
    "        df2 (dataframe): dataframe describing second dataset\n",
    "        patient_col (str): string name of column with patient IDs\n",
    "    \n",
    "    Returns:\n",
    "        leakage (bool): True if there is leakage, otherwise False\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    df1_patients_unique = set(df1[patient_col])\n",
    "    df2_patients_unique = set(df2[patient_col])\n",
    "    \n",
    "    patients_in_both_groups = list(df1_patients_unique.intersection(df2_patients_unique))\n",
    "\n",
    "    # leakage contains true if there is patient overlap, otherwise false.\n",
    "    leakage = len(patients_in_both_groups) > 0 \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'PatientId')))\n",
    "print(\"leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'PatientId')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data labels\n",
    "Create a list of the names of each patient condition or disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_df.keys()\n",
    "columns = list(columns)\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecesary elements\n",
    "columns.remove('Image')\n",
    "columns.remove('PatientId')\n",
    "# Get the total classes\n",
    "print(f\"There are {len(columns)} columns of labels for these conditions: {columns}\")\n",
    "# Print out the number of positive labels for each class\n",
    "for column in columns:\n",
    "    print(f\"The class {column} has {train_df[column].sum()} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "Using the image names listed in the csv file, retrieve the image associated with each row of data in your dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numpy values from Image column in data frame\n",
    "images = train_df['Image'].values\n",
    "\n",
    "# Extract 9 random images from it\n",
    "random_images = [np.random.choice(images) for i in range(9)]\n",
    "\n",
    "# Location of the image dir\n",
    "img_dir = 'nih_new/images-small/'\n",
    "\n",
    "print('Display Random Images')\n",
    "\n",
    "# Adjust the size of your images\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "# Iterate and plot random images\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    img = plt.imread(os.path.join(img_dir, random_images[i]))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    \n",
    "# Adjust subplot parameters to give specified padding\n",
    "plt.tight_layout()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate a single image\n",
    "Look at the first image in the dataset and print out some details of the image contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first image that was listed in the train_df dataframe\n",
    "sample_img = train_df.Image[0]\n",
    "raw_image = plt.imread(os.path.join(img_dir, sample_img))\n",
    "plt.imshow(raw_image, cmap='gray')\n",
    "plt.grid(color='w', linestyle='-', linewidth=1)\n",
    "plt.colorbar()\n",
    "plt.title('Raw Chest X Ray Image')\n",
    "print(f\"The dimensions of the image are {raw_image.shape[0]} pixels width and {raw_image.shape[1]} pixels height, one single color channel\")\n",
    "print(f\"The maximum pixel value is {raw_image.max():.4f} and the minimum is {raw_image.min():.4f}\")\n",
    "print(f\"The mean value of the pixels is {raw_image.mean():.4f} and the standard deviation is {raw_image.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate pixel value distribution\n",
    "Plot up the distribution of pixel values in the image shown above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels = np.reshape(raw_image,raw_image.shape[0]*raw_image.shape[1])\n",
    "plt.hist(pixels, bins=50, label=f'Pixel Mean {np.mean(raw_image):.4f} & Standard Deviation {np.std(raw_image):.4f}')\n",
    "plt.legend(loc='upper center')\n",
    "plt.title('Distribution of Pixel Intensities in the Image')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('# Pixels in Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing in Keras\n",
    "### Standardization\n",
    "\n",
    "Normalizing images is better suited for training a convolutional neural network. For this task we use the Keras [ImageDataGenerator](https://keras.io/preprocessing/image/) function to perform data preprocessing and data augmentation.\n",
    "The `image_generator` will adjust the image data such that the new mean of the data will be zero, and the standard deviation of the data will be 1.  \n",
    "\n",
    "In other words, the generator will replace each pixel value in the image with a new value calculated by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "$$\\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "Create an image generator for preprocessing. Pre-process the data using the `image_generator`as well as reduce the image size down to 320x320 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data generator from keras https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
    "    \"\"\"\n",
    "    Return generator for training set, normalizing using batch\n",
    "    statistics.\n",
    "\n",
    "    Args:\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      batch_size (int): images per batch to be fed into model during training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        train_generator (DataFrameIterator): iterator over training set\n",
    "    \"\"\"        \n",
    "    print(\"getting train generator...\") \n",
    "    # Normalize images  --- Generate batches of tensor image data with real-time data augmentation\n",
    "    image_generator = ImageDataGenerator(\n",
    "        samplewise_center=True,              #Set each sample mean to 0\n",
    "        samplewise_std_normalization= True)  # Divide each input by its standard deviation\n",
    "    \n",
    "    # flow from directory with specified batch size and target image size\n",
    "    # flow_from_dataframe ==> https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "    # RETURNS a DataFrameIterator yielding tuples of (x, y) where x is a numpy array containing a batch of images with \n",
    "    # shape (batch_size, *target_size, channels) and y is a numpy array of corresponding labels\n",
    "    # default data format of ImageGenerator is channels_last\n",
    "    generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",       #  Mode for yielding the targets, one of \"binary\", \"categorical\", \"input\", \"multi_output\", \"raw\", sparse\" or None. Default: \"categorical\".\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=8, \n",
    "                                 seed=1, target_w = 320, target_h = 320):\n",
    "    \"\"\"\n",
    "    Return generator for validation set and test test set using \n",
    "    normalization statistics from training set.\n",
    "\n",
    "    Args:\n",
    "      valid_df (dataframe): dataframe specifying validation data.\n",
    "      test_df (dataframe): dataframe specifying test data.\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      sample_size (int): size of sample to use for normalization statistics.\n",
    "      batch_size (int): images per batch to be fed into model during training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n",
    "    \"\"\"\n",
    "    # get generator to sample dataset\n",
    "    print(f\"\\nextracting {sample_size} train images to normalize validation and test datasets...\")\n",
    "\n",
    "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "        dataframe=train_df, \n",
    "        directory=IMAGE_DIR, \n",
    "        x_col=\"Image\", \n",
    "        y_col=labels, \n",
    "        class_mode=\"raw\", \n",
    "        batch_size=sample_size, \n",
    "        shuffle=True, \n",
    "        target_size=(target_w, target_h))\n",
    "    \n",
    "    # get data sample\n",
    "    batch = raw_train_generator.next() # generate a batch of samples and associated labels \n",
    "    data_sample = batch[0]             # => we need only the sample imgs ie batch[0]\n",
    "\n",
    "    # use sample to fit mean and std for test set generator\n",
    "    image_generator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization= True)\n",
    "    \n",
    "    # fit generator to sample from training data - we use this generator normalizing mean and std using the train sample of 100\n",
    "    image_generator.fit(data_sample)\n",
    "    \n",
    "    print(\"\\ngetting valid generator...\")\n",
    "\n",
    "    # get test generator\n",
    "    valid_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    \n",
    "    print(\"\\ngetting test generator...\")\n",
    "    test_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    return valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis',\n",
    "              'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema', 'Consolidation']\n",
    "IMAGE_DIR = \"nih_new/images-small/\"\n",
    "train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\", labels)\n",
    "valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"Image\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a processed image\n",
    "sns.set_style(\"white\")\n",
    "generated_image, label = train_generator.__getitem__(0)\n",
    "plt.imshow(generated_image[0], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.title('Raw Chest X Ray Image')\n",
    "print(f\"The dimensions of the image are {generated_image.shape[1]} pixels width and {generated_image.shape[2]} pixels height\")\n",
    "print(f\"The maximum pixel value is {generated_image.max():.4f} and the minimum is {generated_image.min():.4f}\")\n",
    "print(f\"The mean value of the pixels is {generated_image.mean():.4f} and the standard deviation is {generated_image.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_image.shape, generated_image[0].shape)\n",
    "print(raw_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a histogram of the distribution of the pixels\n",
    "sns.set()\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot histogram for original iamge\n",
    "sns.distplot(raw_image.ravel(), \n",
    "             label=f'Original Image: mean {np.mean(raw_image):.4f} - Standard Deviation {np.std(raw_image):.4f} \\n '\n",
    "             f'Min pixel value {np.min(raw_image):.4} - Max pixel value {np.max(raw_image):.4}',\n",
    "             color='blue', \n",
    "             kde=False)\n",
    "\n",
    "# Plot histogram for generated image\n",
    "sns.distplot(generated_image[0].ravel(), \n",
    "             label=f'Generated Image: mean {np.mean(generated_image[0]):.4f} - Standard Deviation {np.std(generated_image[0]):.4f} \\n'\n",
    "             f'Min pixel value {np.min(generated_image[0]):.4} - Max pixel value {np.max(generated_image[0]):.4}', \n",
    "             color='red', \n",
    "             kde=False)\n",
    "\n",
    "# Place legends\n",
    "plt.legend()\n",
    "plt.title('Distribution of Pixel Intensities in the Image')\n",
    "plt.xlabel('Pixel Intensity')\n",
    "plt.ylabel('# Pixel')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Class Imbalance - Weighted Loss\n",
    "One of the challenges with working with medical diagnostic datasets is the large class imbalance present in such datasets. Let's plot the frequency of each of the labels in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xticks(rotation=90)\n",
    "plt.bar(x=labels, height=np.mean(train_generator.labels, axis=0))\n",
    "plt.title(\"Frequency of Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this plot that the prevalance of positive cases varies significantly across the different pathologies. (These trends mirror the ones in the full dataset as well.) \n",
    "* The `Hernia` pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. \n",
    "* But even the `Infiltration` pathology, which has the least amount of imbalance, has only 17.5% of the training cases labelled positive.\n",
    "\n",
    "Ideally, we would train our model using an evenly balanced dataset so that the positive and negative training cases would contribute equally to the loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impact of class imbalance on loss function\n",
    "\n",
    "Let's take a closer look at this. Assume we would have used a normal cross-entropy loss for each pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is:\n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$\n",
    "\n",
    "where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive. \n",
    "\n",
    "Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms contributes to the loss (the other term is multiplied by zero, and becomes zero). \n",
    "\n",
    "We can rewrite the overall average cross-entropy loss over the entire training set $\\mathcal{D}$ of size $N$ as follows: \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum_{\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$\n",
    "\n",
    "Using this formulation, we can see that if there is a large imbalance with very few positive training cases, for example, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: \n",
    "\n",
    "$$freq_{p} = \\frac{\\text{number of positive examples}}{N} $$\n",
    "\n",
    "$$\\text{and}$$\n",
    "\n",
    "$$freq_{n} = \\frac{\\text{number of negative examples}}{N}.$$\n",
    "\n",
    "### Computing Class Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_freqs(labels):\n",
    "    \"\"\"\n",
    "    Compute positive and negative frequences for each class.\n",
    "\n",
    "    Args:\n",
    "        labels (np.array): matrix of labels, size (num_examples, num_classes)\n",
    "    Returns:\n",
    "        positive_frequencies (np.array): array of positive frequences for each\n",
    "                                         class, size (num_classes)\n",
    "        negative_frequencies (np.array): array of negative frequences for each\n",
    "                                         class, size (num_classes)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # total number of patients (rows)\n",
    "    N = labels.shape[0]\n",
    "    \n",
    "    positive_frequencies = np.sum(labels, axis=0)/N\n",
    "    negative_frequencies = (N - np.sum(labels, axis=0))/N  # broadcasting of N to a line vector of dim num_classes\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\n",
    "freq_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\n",
    "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n",
    "plt.xticks(rotation=90)\n",
    "f = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. \n",
    "\n",
    "To have this, we want \n",
    "\n",
    "$$w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n},$$\n",
    "\n",
    "which we can do simply by taking \n",
    "\n",
    "$$w_{pos} = freq_{neg}$$\n",
    "$$w_{neg} = freq_{pos}$$\n",
    "\n",
    "This way, we will be balancing the contribution of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weights = freq_neg\n",
    "neg_weights = freq_pos\n",
    "pos_contribution = freq_pos * pos_weights \n",
    "neg_contribution = freq_neg * neg_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify this by graphing the two contributions next to each other :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\n",
    "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n",
    "                        for l,v in enumerate(neg_contribution)], ignore_index=True)\n",
    "plt.xticks(rotation=90)\n",
    "sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the weights, our final weighted loss for each training case will be \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Return weighted loss function given negative weights and positive weights.\n",
    "\n",
    "    Args:\n",
    "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
    "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
    "    \n",
    "    Returns:\n",
    "      weighted_loss (function): weighted loss function\n",
    "    \"\"\"\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return weighted loss value. \n",
    "\n",
    "        Args:\n",
    "            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n",
    "            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n",
    "        Returns:\n",
    "            loss (Float): overall scalar loss summed across all classes\n",
    "        \"\"\"\n",
    "        # initialize loss to zero\n",
    "        loss = 0.0\n",
    "        \n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "        for i in range(len(pos_weights)):\n",
    "            # for each class, add average weighted loss for that class \n",
    "            loss += - pos_weights[i] * K.mean(y_true[:,i] * K.log(y_pred[:,i] + epsilon)) \\\n",
    "            - neg_weights[i] * K.mean((1-y_true[:,i]) * K.log(1-y_pred[:,i] + epsilon)) #complete this line\n",
    "        return loss\n",
    "    \n",
    "        ### END CODE HERE ###\n",
    "    return weighted_loss             # this is a function taking 2 arguments y_true and y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DenseNet121\n",
    "Use a pre-trained [DenseNet121](https://www.kaggle.com/pytorch/densenet121) model.Densenet is a convolutional network where each layer is connected to all other layers that are deeper in the network\n",
    "- The first layer is connected to the 2nd, 3rd, 4th etc.\n",
    "- The second layer is connected to the 3rd, 4th, 5th etc.\n",
    "- For a detailed explanation of Densenet, check out the source of the image above, a paper by Gao Huang et al. 2018 called [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf).\n",
    "<img src=\"densenet.png\" alt=\"U-net Image\" width=\"400\" align=\"middle\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can load directly from Keras and then add two layers on top of it:\n",
    "1. A `GlobalAveragePooling2D` layer to get the average of the last convolution layers from DenseNet121.\n",
    "2. A `Dense` layer with `sigmoid` activation to get the prediction logits for each of our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
    "from keras.preprocessing import image\n",
    "\n",
    "def load_C3M3_model():\n",
    "   \n",
    "    class_pos = train_df.loc[:, labels].sum(axis=0)\n",
    "    class_neg = len(train_df) - class_pos\n",
    "    class_total = class_pos + class_neg\n",
    "\n",
    "    pos_weights = class_pos / class_total\n",
    "    neg_weights = class_neg / class_total\n",
    "    print(\"Got loss weights\")\n",
    "    # create the base pre-trained model\n",
    "    base_model = DenseNet121(weights='densenet.hdf5', include_top=False)\n",
    "    print(\"Loaded DenseNet\")\n",
    "    # add a global spatial average pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # and a logistic layer\n",
    "    predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n",
    "    print(\"Added layers\")\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss=get_weighted_loss(neg_weights, pos_weights))\n",
    "    print(\"Compiled Model\")\n",
    "\n",
    "    model.load_weights(\"nih_new/pretrained_model.h5\")\n",
    "    print(\"Loaded Weights\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_C3M3_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see the layers that our model is composed of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the total number of layers\n",
    "layers_ = model.layers\n",
    "print('total number of layers =',len(layers_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find() method returns an integer value:\n",
    "# If substring doesn't exist inside the string, it returns -1, otherwise returns first occurence index\n",
    "conv2D_layers = [layer for layer in model.layers \n",
    "                if str(type(layer)).find('Conv2D') > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model input -------------->', model.input)\n",
    "print('Feature extractor output ->', model.get_layer('conv5_block16_concat').output)\n",
    "print('Model output ------------->', model.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=valid_generator,\n",
    "                              steps_per_epoch=100, \n",
    "                              validation_steps=25, \n",
    "                              epochs = 1)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Evaluation\n",
    "Now that we have a model, let's evaluate it using our test set. We can conveniently use the predict_generator function to generate the predictions for the images in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_vals = model.predict_generator(test_generator, steps = len(test_generator))\n",
    "predicted_vals.shape  # number of test samples x number of classes to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUROC\n",
    "Compute metric called the AUC (Area Under the Curve) from the ROC ([Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) curve. ideally we want a curve that is more to the left so that the top has more \"area\" under it, which indicates that the model is performing better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_roc_curve(labels, predicted_vals, generator):\n",
    "    auc_roc_vals = []\n",
    "    for i in range(len(labels)):\n",
    "        try:\n",
    "            gt = generator.labels[:, i]\n",
    "            pred = predicted_vals[:, i]\n",
    "            auc_roc = roc_auc_score(gt, pred)\n",
    "            auc_roc_vals.append(auc_roc)\n",
    "            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n",
    "            plt.figure(1, figsize=(10, 10))\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr_rf, tpr_rf,\n",
    "                     label=labels[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "        except:\n",
    "            print(\n",
    "                f\"Error in generating ROC curve for {labels[i]}. \"\n",
    "                f\"Dataset lacks enough examples.\"\n",
    "            )\n",
    "    plt.savefig('ROC.png')\n",
    "    plt.show()\n",
    "    return auc_roc_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_rocs = get_roc_curve(labels, predicted_vals, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"areas under the curve : {} \\n for all {} classes\".format(auc_rocs,len(auc_rocs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RaIViDj8khSg"
   },
   "source": [
    "## Interpreting Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B07VP7edyb98"
   },
   "source": [
    "Let's load in an X-ray image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "colab_type": "code",
    "id": "cVVXgMweyGtz",
    "outputId": "37992056-b6ea-4316-a44d-6c98b7074423",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "\n",
    "def get_mean_std_per_batch(df, H=320, W=320):\n",
    "    sample_data = []\n",
    "    for idx, img in enumerate(df.sample(100)[\"Image\"].values):\n",
    "        path = IMAGE_DIR + img\n",
    "        sample_data.append(np.array(image.load_img(path, target_size=(H, W))))\n",
    "\n",
    "    mean = np.mean(sample_data[0])\n",
    "    std = np.std(sample_data[0])\n",
    "    return mean, std    \n",
    "\n",
    "def load_image_normalize(path, mean, std, H=320, W=320):\n",
    "    x = image.load_img(path, target_size=(H, W))\n",
    "    x -= mean\n",
    "    x /= std\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "def load_image(path, df, preprocess=True, H = 320, W = 320):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    x = image.load_img(path, target_size=(H, W))\n",
    "    if preprocess:\n",
    "        mean, std = get_mean_std_per_batch(df, H=H, W=W)\n",
    "        x -= mean\n",
    "        x /= std\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "im_path = IMAGE_DIR + '00025288_001.png' \n",
    "x = load_image(im_path, train_df, preprocess=False)\n",
    "plt.imshow(x, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1N1_YANDztAo"
   },
   "source": [
    "Next, let's get our predictions. Before we plug the image into our model, we have to normalize it. Run the next cell to compute the mean and standard deviation of the images in our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5I91kMHxUiz"
   },
   "outputs": [],
   "source": [
    "mean, std = get_mean_std_per_batch(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IGX2R05t6ZLA"
   },
   "source": [
    "Now we are ready to normalize and run the image through our model to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "colab_type": "code",
    "id": "-UG6DAnUzxk0",
    "outputId": "13fedfec-3aed-4830-e435-827bb1de1a9d"
   },
   "outputs": [],
   "source": [
    "labels = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis',\n",
    "              'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema', 'Consolidation']\n",
    "\n",
    "processed_image = load_image_normalize(im_path, mean, std)\n",
    "preds = model.predict(processed_image)\n",
    "pred_df = pd.DataFrame(preds, columns = labels)\n",
    "pred_df.loc[0, :].plot.bar()\n",
    "plt.title(\"Predictions\")\n",
    "plt.savefig('predictions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpNVTzl6002K"
   },
   "source": [
    "We see, for example, that the model predicts Mass (abnormal spot or area in the lungs that are more than 3 centimeters) with high probability. Indeed, this patient was diagnosed with mass. However, we don't know where the model is looking when it's making its own diagnosis. To gain more insight into what the model is looking at, we can use GradCAMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKeH6_eBDCho"
   },
   "source": [
    "# GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradCAM is a technique to visualize the impact of each region of an image on a specific output for a Convolutional Neural Network model. Through GradCAM, we can generate a heatmap by computing gradients of the specific class scores we are interested in visualizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbKwnYWRHmxa"
   },
   "source": [
    "#### Getting Intermediate Layers\n",
    "\n",
    "Perhaps the most complicated part of computing GradCAM is accessing intermediate activations in our deep learning model and computing gradients with respect to the class output. Typically we'll only be extracting one of the last few. The last few layers usually have more abstract information. To access a layer, we can use `model.get_layer(layer).output`, which takes in the name of the layer in question. Here we are going to extract the raw output of the last convolutional layer `conv5_block16_concat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "icUrvQF7KUJp",
    "outputId": "611d75e5-1345-489f-a01a-bb566ae230cf"
   },
   "outputs": [],
   "source": [
    "spatial_maps =  model.get_layer('conv5_block16_concat').output\n",
    "print(spatial_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCsIMf08KrWi"
   },
   "source": [
    "Now, this tensor is just a placeholder, it doesn't contain the actual activations for a particular image. To get this we will use [Keras.backend.function](https://www.tensorflow.org/api_docs/python/tf/keras/backend/function) to return intermediate computations while the model is processing a particular input. This method takes in an input and output placeholders and returns a function. This function will compute the intermediate output (until it reaches the given placeholder) evaluated given the input. For example, if you want the layer that you just retrieved (conv5_block16_concat), you could write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U4JEE37ALl-N",
    "outputId": "1de337cf-bec5-4edf-fb6d-4bbbe672bad8"
   },
   "outputs": [],
   "source": [
    "get_spatial_maps = K.function([model.input], [spatial_maps])\n",
    "print(get_spatial_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B2-soRaiL8xA"
   },
   "source": [
    "We see that we now have a `Function` object. Now, to get the actual intermediate output evaluated with a particular input, we just plug in an image to this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an image\n",
    "x = load_image_normalize(im_path, mean, std)\n",
    "print(f\"x is of type {type(x)}\")\n",
    "print(f\"x is of shape {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 0th item in the list\n",
    "spatial_maps_x = get_spatial_maps([x])[0]\n",
    "print(f\"spatial_maps_x is of type {type(spatial_maps_x)}\")\n",
    "print(f\"spatial_maps_x is of shape {spatial_maps_x.shape}\")\n",
    "print(f\"spatial_maps_x without the batch dimension has shape {spatial_maps_x[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRCSdXfRO6KI"
   },
   "source": [
    "We now have the activations for that particular image, and we can use it for interpretation. The function that is returned by calling `K.function([model.input], [spatial_maps])` (saved here in the variable `get_spatial_maps`) is sometimes referred to as a \"hook\", letting you peek into the intermediate computations in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJlxNWRyPQqV"
   },
   "source": [
    "#### Getting Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G12g9fOeaqyM"
   },
   "source": [
    "The other major step in computing GradCAMs is getting gradients with respect to the output for a particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output of the model\n",
    "output_with_batch_dim = model.output\n",
    "print(f\"Model output includes batch dimension, has shape {output_with_batch_dim.shape}\")\n",
    "print(f\"excluding the batch dimension, the output for all 14 categories of disease has shape {output_with_batch_dim[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output has 14 categories, one for each disease category, indexed from 0 to 13. Cardiomegaly is the disease category at index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first category's output (Cardiomegaly) at index 0\n",
    "y_category_0 = output_with_batch_dim[0][0]\n",
    "print(f\"The Cardiomegaly output is at index 0, and has shape {y_category_0.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gdnX8taUbF7h",
    "outputId": "646e7c7c-0ab6-492d-f6b5-26b30f4c1125"
   },
   "outputs": [],
   "source": [
    "# Get gradient of y_category_0 with respect to spatial_maps\n",
    "# The first parameter is the value you are taking the gradient of, and the second is the parameter you are taking that gradient with respect to.\n",
    "gradient_l = K.gradients(y_category_0, spatial_maps)\n",
    "print(f\"gradient_l is of type {type(gradient_l)} and has length {len(gradient_l)}\")\n",
    "\n",
    "# gradient_l is a list of size 1.  Get the gradient at index 0\n",
    "gradient = gradient_l[0]\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fj-b8lqHbrM5"
   },
   "source": [
    "Again, this is just a placeholder. Just like for intermediate layers, we can use `K.function` to compute the value of the gradient for a particular input.  \n",
    "\n",
    "The K.function() takes in\n",
    "- a list of inputs: in this case, one input, 'model.input'\n",
    "- a list of tensors: in this case, one output tensor 'gradient'\n",
    "\n",
    "It returns a function that calculates the activations of the list of tensors.\n",
    "- This returned function returns a list of the activations, one for each tensor that was passed into K.function()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function that gets the gradient\n",
    "get_gradient = K.function([model.input], [gradient])\n",
    "type(get_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an input x-ray image\n",
    "x = load_image_normalize(im_path, mean, std)\n",
    "print(f\"X-ray image has shape {x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_gradient` function takes in a list of inputs, and returns a list of the gradients, one for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the get_gradient function to get the gradient (pass in the input image inside a list)\n",
    "grad_x_l = get_gradient([x])\n",
    "print(f\"grad_x_l is of type {type(grad_x_l)} and length {len(grad_x_l)}\")\n",
    "\n",
    "# get the gradient at index 0 of the list.\n",
    "grad_x_with_batch_dim = grad_x_l[0]\n",
    "print(f\"grad_x_with_batch_dim is type {type(grad_x_with_batch_dim)} and shape {grad_x_with_batch_dim.shape}\")\n",
    "\n",
    "# To remove the batch dimension, take the value at index 0 of the batch dimension\n",
    "grad_x = grad_x_with_batch_dim[0]\n",
    "print(f\"grad_x is type {type(grad_x)} and shape {grad_x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdLxNcp9dD3i"
   },
   "source": [
    "### Implementing GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYwp5kvT8ZfR"
   },
   "outputs": [],
   "source": [
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    \"\"\"\n",
    "    GradCAM method for visualizing input saliency.\n",
    "    \n",
    "    Args:\n",
    "        input_model (Keras.model): model to compute cam for\n",
    "        image (tensor): input to model, shape (1, H, W, 3)\n",
    "        cls (int): class to compute cam with respect to\n",
    "        layer_name (str): relevant layer in model\n",
    "        H (int): input height\n",
    "        W (int): input width\n",
    "    Return:\n",
    "        cam ()\n",
    "    \"\"\"\n",
    "    cam = None\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "\n",
    "    # 1. Get placeholders for class output and last layer\n",
    "    # Get the model's output\n",
    "    output_with_batch_dim = input_model.output\n",
    "    \n",
    "    # Remove the batch dimension\n",
    "    output_all_categories = output_with_batch_dim[0]\n",
    "    \n",
    "    # Retrieve only the disease category at the given category index\n",
    "    y_c = output_all_categories[category_index]\n",
    "    \n",
    "    # Get the input model's layer specified by layer_name, and retrive the layer's output tensor\n",
    "    spatial_map_layer = input_model.get_layer(layer_name).output\n",
    "\n",
    "    # 2. Get gradients of last layer with respect to output\n",
    "\n",
    "    # get the gradients of y_c with respect to the spatial map layer (it's a list of length 1)\n",
    "    grads_l = K.gradients(y_c, spatial_map_layer)\n",
    "    \n",
    "    # Get the gradient at index 0 of the list\n",
    "    grads = grads_l[0]\n",
    "        \n",
    "    # 3. Get hook for the selected layer and its gradient, based on given model's input\n",
    "    # Hint: Use the variables produced by the previous two lines of code\n",
    "    spatial_map_and_gradient_function = K.function([input_model.input], [spatial_map_layer, grads])\n",
    "    \n",
    "    # Put in the image to calculate the values of the spatial_maps (selected layer) and values of the gradients\n",
    "    spatial_map_all_dims, grads_val_all_dims = spatial_map_and_gradient_function([image])\n",
    "\n",
    "    # Reshape activations and gradient to remove the batch dimension\n",
    "    # Shape goes from (B, H, W, C) to (H, W, C)\n",
    "    # B: Batch. H: Height. W: Width. C: Channel    \n",
    "    # Reshape spatial map output to remove the batch dimension\n",
    "    spatial_map_val = spatial_map_all_dims[0]\n",
    "    \n",
    "    # Reshape gradients to remove the batch dimension\n",
    "    grads_val = grads_val_all_dims[0]\n",
    "    \n",
    "    # 4. Compute weights using global average pooling on gradient \n",
    "    # grads_val has shape (Height, Width, Channels) (H,W,C)\n",
    "    # Take the mean across the height and also width, for each channel\n",
    "    # Make sure weights have shape (C)\n",
    "    weights = np.mean(grads_val, axis=(0,1))\n",
    "    \n",
    "    # 5. Compute dot product of spatial map values with the weights\n",
    "    cam = np.dot(spatial_map_val, weights)    # shape (10,10,1024) x shape(1024,) resulting into shape(10,10)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # We'll take care of the postprocessing.\n",
    "    H, W = image.shape[1], image.shape[2]\n",
    "    cam = np.maximum(cam, 0) # ReLU so we only get positive importance\n",
    "    cam = cv2.resize(cam, (W, H), cv2.INTER_NEAREST)\n",
    "    cam = cam / cam.max()\n",
    "\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBi4c71M7OVY"
   },
   "source": [
    "- Generate the CAM for the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4yAC5xBo8J3L",
    "outputId": "0bb46d16-07c6-4211-8312-5b3de8ba06a7"
   },
   "outputs": [],
   "source": [
    "im = load_image_normalize(im_path, mean, std)\n",
    "cam = grad_cam(model, im, 5, 'conv5_block16_concat') # Mass is class 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tnl0D5pN8MvE"
   },
   "source": [
    "- Visualize the CAM and the original image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "colab_type": "code",
    "id": "m1kqthIt5AOs",
    "outputId": "5d486156-ada6-4670-fe70-ae928b7baef3"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "plt.subplot(121)\n",
    "plt.imshow(load_image(im_path, train_df, preprocess=False), cmap='gray')\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(load_image(im_path, train_df, preprocess=False), cmap='gray')\n",
    "plt.imshow(cam, cmap='magma', alpha=0.5)\n",
    "plt.title(\"GradCAM\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Lx0lCu-5DeF"
   },
   "source": [
    "We can see that it focuses on the large (white) empty area on the right lung. Indeed this is a clear case of Mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using GradCAM to Visualize Multiple Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUQSkaHsBsUn"
   },
   "source": [
    "We can use GradCAMs for multiple labels on the same image. Let's do it for the labels with best AUC for our model, Cardiomegaly, Mass, and Edema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradcam(model, img, mean, std, data_dir, df, \n",
    "                    labels, selected_labels, layer_name='conv5_block16_concat'):\n",
    "    \"\"\"\n",
    "    Compute GradCAM for many specified labels for an image. \n",
    "    This method will use the `grad_cam` function.\n",
    "    \n",
    "    Args:\n",
    "        model (Keras.model): Model to compute GradCAM for\n",
    "        img (string): Image name we want to compute GradCAM for.\n",
    "        mean (float): Mean to normalize to image.\n",
    "        std (float): Standard deviation to normalize the image.\n",
    "        data_dir (str): Path of the directory to load the images from.\n",
    "        df(pd.Dataframe): Dataframe with the image features.\n",
    "        labels ([str]): All output labels for the model.\n",
    "        selected_labels ([str]): All output labels we want to compute the GradCAM for.\n",
    "        layer_name: Intermediate layer from the model we want to compute the GradCAM for.\n",
    "    \"\"\"\n",
    "    img_path = data_dir + img\n",
    "    preprocessed_input = load_image_normalize(img_path, mean, std)\n",
    "    predictions = model.predict(preprocessed_input)\n",
    "    print(\"Ground Truth: \", \", \".join(np.take(labels, np.nonzero(df[df[\"Image\"] == img][labels].values[0]))[0]))\n",
    "\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    plt.subplot(151)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(load_image(img_path, df, preprocess=False), cmap='gray')\n",
    "    \n",
    "    j = 1\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###    \n",
    "    # Loop through all labels\n",
    "    for i in range(len(labels)): # complete this line\n",
    "        # Compute CAM and show plots for each selected label.\n",
    "        \n",
    "        # Check if the label is one of the selected labels\n",
    "        if labels[i] in selected_labels: # complete this line\n",
    "            \n",
    "            # Use the grad_cam function to calculate gradcam\n",
    "            gradcam = grad_cam(model, preprocessed_input, i, layer_name)\n",
    "            \n",
    "            ### END CODE HERE ###\n",
    "            \n",
    "            print(\"Generating gradcam for class %s (p=%2.2f)\" % (labels[i], round(predictions[0][i], 3)))\n",
    "            plt.subplot(151 + j)\n",
    "            plt.title(labels[i] + \": \" + str(round(predictions[0][i], 3)))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(load_image(img_path, df, preprocess=False), cmap='gray')\n",
    "            plt.imshow(gradcam, cmap='magma', alpha=min(0.5, predictions[0][i]))\n",
    "            j +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIB7qyZ6_eu4"
   },
   "source": [
    "Run the following cells to print the ground truth diagnosis for a given case and show the original x-ray as well as GradCAMs for Cardiomegaly, Mass, and Edema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "zCh0tNn_6Wmu",
    "outputId": "26e0692b-af47-46f0-f749-8e007e252fba"
   },
   "outputs": [],
   "source": [
    "image_filename = '00016650_000.png'\n",
    "labels_to_show = ['Cardiomegaly', 'Mass', 'Edema']\n",
    "compute_gradcam(model, image_filename, mean, std, IMAGE_DIR, train_df, labels, labels_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cvPOHJeb6HkD"
   },
   "source": [
    "The model correctly predicts absence of mass or edema. The probability for mass is higher, and we can see that it may be influenced by the shapes in the middle of the chest cavity, as well as around the shoulder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "4eiXeIXWCCTf",
    "outputId": "90838ee8-7843-4a7f-9268-ad6b36833b94"
   },
   "outputs": [],
   "source": [
    "image_filename = '00005410_000.png'\n",
    "compute_gradcam(model, image_filename, mean, std, IMAGE_DIR, train_df, labels, labels_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8Wx5u3t6fiE"
   },
   "source": [
    "In the example above, the model correctly focuses on the mass near the center of the chest cavity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "id": "V5ViYPuIqM3H",
    "outputId": "1bf4aa59-bb4a-49ad-8b8a-eb654a97dcaa"
   },
   "outputs": [],
   "source": [
    "image_name = '00004090_002.png'\n",
    "compute_gradcam(model, image_name, mean, std, IMAGE_DIR, train_df, labels, labels_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FroGFoB98A26"
   },
   "source": [
    "Here the model correctly picks up the signs of edema near the bottom of the chest cavity. We can also notice that Cardiomegaly has a high score for this image, though the ground truth doesn't include it.\n",
    "This visualization might be helpful for error analysis; for example, we can notice that the model is indeed looking at the expected area to make the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = '00025288_001.png'\n",
    "compute_gradcam(model, image_name, mean, std, IMAGE_DIR, train_df, labels, labels_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sN_laVv3DHVp"
   },
   "source": [
    "Here the model picks up signs of the mass near the center of the chest cavity on the right. Edema has a high score for this image, though the ground truth doesn't include it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation tools like this one can be helpful for discovery of markers, error analysis, and even in deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "C3M2_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "AI4MC3-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
